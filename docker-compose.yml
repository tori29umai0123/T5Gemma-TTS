services:
  t5gemma-tts:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # CUDA version: cu118, cu121, cu124, cu128 (default)
        CUDA_VERSION: ${CUDA_VERSION:-cu128}
    ports:
      - "${PORT:-7860}:7860"
    volumes:
      # Persist HuggingFace cache to avoid re-downloading models
      - hf_cache:/app/.cache/huggingface
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Override default command if MODEL_DIR is set
    command: >
      python inference_gradio.py
      --model_dir ${MODEL_DIR:-Aratako/T5Gemma-TTS-2b-2b}
      --port 7860
      ${EXTRA_ARGS:-}

volumes:
  hf_cache:
